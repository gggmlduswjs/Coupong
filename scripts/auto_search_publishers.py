"""ì¶œíŒì‚¬ë³„ ìµœì‹  ë„ì„œ ìë™ ê²€ìƒ‰ & ë“±ë¡"""
import sys
from pathlib import Path
import os
from datetime import datetime, timedelta

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from crawlers.aladin_api_crawler import AladinAPICrawler
from app.database import SessionLocal
from app.models.kyobo_product import KyoboProduct
from app.models.account import Account
from uploaders.coupang_csv_generator import CoupangCSVGenerator
from config.publishers import PUBLISHERS, get_publisher_info, meets_free_shipping


def main():
    """ì¶œíŒì‚¬ë³„ ìµœì‹  ë„ì„œ ìë™ ê²€ìƒ‰"""

    print("\n" + "ğŸš€ "*30)
    print("ì¶œíŒì‚¬ë³„ ìµœì‹  ë„ì„œ ìë™ ê²€ìƒ‰ ì‹œìŠ¤í…œ")
    print("ğŸš€ "*30)

    # TTBKey í™•ì¸
    ttb_key = os.getenv("ALADIN_TTB_KEY")

    if not ttb_key:
        print("\nâš ï¸  ì•Œë¼ë”˜ TTBKeyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        print("ALADIN_API_GUIDE.md ì°¸ê³ í•˜ì—¬ ë°œê¸‰ë°›ìœ¼ì„¸ìš”.")
        return

    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    crawler = AladinAPICrawler(ttb_key=ttb_key)

    # ê²€ìƒ‰ ì„¤ì •
    print("\n" + "="*60)
    print("ê²€ìƒ‰ ì„¤ì •")
    print("="*60)

    print(f"\nì·¨ê¸‰ ì¶œíŒì‚¬: {len(PUBLISHERS)}ê°œ")
    print("\nì£¼ìš” ì¶œíŒì‚¬:")
    for pub in PUBLISHERS[:10]:
        print(f"  - {pub['name']} (ë§¤ì…ë¥  {pub['margin']}%)")
    if len(PUBLISHERS) > 10:
        print(f"  ... ì™¸ {len(PUBLISHERS) - 10}ê°œ")

    # ê²€ìƒ‰ ì˜µì…˜
    print("\nê²€ìƒ‰ ëª¨ë“œ:")
    print("1. ì „ì²´ ì¶œíŒì‚¬ ìë™ ê²€ìƒ‰ (ê¶Œì¥)")
    print("2. íŠ¹ì • ì¶œíŒì‚¬ë§Œ ì„ íƒ")

    mode = input("\nì„ íƒ (1 or 2): ").strip() or "1"

    if mode == "2":
        publishers_to_search = select_publishers()
    else:
        publishers_to_search = PUBLISHERS

    # ì¶œíŒì‚¬ë‹¹ ê²€ìƒ‰ ê°œìˆ˜
    max_per_publisher = input("\nì¶œíŒì‚¬ë‹¹ ìµœëŒ€ ê²€ìƒ‰ ê°œìˆ˜ (ê¸°ë³¸ 10): ").strip()
    try:
        max_per_publisher = int(max_per_publisher) if max_per_publisher else 10
    except:
        max_per_publisher = 10

    # ìµœì‹ ë„ì„œ í•„í„° (ì¶œê°„ì¼ ê¸°ì¤€)
    days_back = input("\nìµœê·¼ ë©°ì¹  ì´ë‚´ ì¶œê°„ ë„ì„œ? (ê¸°ë³¸ 365ì¼): ").strip()
    try:
        days_back = int(days_back) if days_back else 365
    except:
        days_back = 365

    cutoff_date = datetime.now() - timedelta(days=days_back)

    # ê²€ìƒ‰ ì‹œì‘
    print("\n" + "="*60)
    print("ì•Œë¼ë”˜ API ê²€ìƒ‰ ì¤‘...")
    print("="*60)

    all_products = []
    filtered_products = []

    for pub in publishers_to_search:
        pub_name = pub["name"]
        print(f"\nğŸ” [{pub_name}] ê²€ìƒ‰ ì¤‘...")

        # ì¶œíŒì‚¬ ì´ë¦„ìœ¼ë¡œ ê²€ìƒ‰
        products = crawler.search_by_keyword(pub_name, max_results=max_per_publisher)

        for p in products:
            # ì¶œíŒì‚¬ í•„í„°ë§ (ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²ƒë§Œ)
            if pub_name not in p.get("publisher", ""):
                continue

            # ìµœì‹ ë„ì„œ í•„í„°ë§
            pub_date = p.get("publish_date")
            if pub_date and pub_date < cutoff_date.date():
                continue

            # ë¬´ë£Œë°°ì†¡ ê¸°ì¤€ ì²´í¬
            sale_price = int(p["original_price"] * 0.9)
            if not meets_free_shipping(pub_name, sale_price):
                print(f"   â­ï¸  ë¬´ë£Œë°°ì†¡ ë¯¸ì¶©ì¡±: {p['title'][:30]} ({sale_price:,}ì› < {pub['min_free_shipping']:,}ì›)")
                continue

            all_products.append(p)
            filtered_products.append(p)
            print(f"   âœ“ {p['title'][:40]} ({p['original_price']:,}ì›)")

        print(f"   ê²°ê³¼: {len([p for p in filtered_products if pub_name in p['publisher']])}ê°œ")

    # ê²°ê³¼ ìš”ì•½
    print("\n" + "="*60)
    print("ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½")
    print("="*60)
    print(f"\nì´ ê²€ìƒ‰: {len(all_products)}ê°œ")
    print(f"í•„í„°ë§ í›„: {len(filtered_products)}ê°œ")

    if not filtered_products:
        print("\nì¡°ê±´ì— ë§ëŠ” ë„ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ë¯¸ë¦¬ë³´ê¸°
    print("\n" + "="*60)
    print("ê²€ìƒ‰ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸° (ìƒìœ„ 10ê°œ)")
    print("="*60)

    for i, p in enumerate(filtered_products[:10], 1):
        pub_info = get_publisher_info(p['publisher'])
        sale_price = int(p['original_price'] * 0.9)

        print(f"\n{i}. {p['title'][:50]}")
        print(f"   ì¶œíŒì‚¬: {p['publisher']} (ë§¤ì…ë¥  {pub_info['margin']}%)")
        print(f"   ì •ê°€: {p['original_price']:,}ì› â†’ íŒë§¤ê°€: {sale_price:,}ì›")
        print(f"   ë¬´ë£Œë°°ì†¡: {'âœ“' if meets_free_shipping(p['publisher'], sale_price) else 'âœ—'}")

    if len(filtered_products) > 10:
        print(f"\n... ì™¸ {len(filtered_products) - 10}ê°œ")

    # í™•ì¸
    proceed = input("\n\nDB ì €ì¥ ë° CSV ìƒì„±í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()

    if proceed != 'y':
        print("ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return

    # DB ì €ì¥
    save_to_db(filtered_products)

    # CSV ìƒì„±
    generate_csvs(filtered_products)


def select_publishers():
    """ì¶œíŒì‚¬ ì„ íƒ (ëŒ€í™”í˜•)"""
    print("\n" + "="*60)
    print("ì¶œíŒì‚¬ ì„ íƒ")
    print("="*60)

    for i, pub in enumerate(PUBLISHERS, 1):
        print(f"{i:2d}. {pub['name']:20s} (ë§¤ì…ë¥  {pub['margin']}%)")

    print("\nì„ íƒ ë°©ë²•:")
    print("- ë²ˆí˜¸ ì…ë ¥ (ì˜ˆ: 1,3,5)")
    print("- 'all' ì…ë ¥í•˜ë©´ ì „ì²´ ì„ íƒ")

    choice = input("\nì„ íƒ: ").strip()

    if choice.lower() == 'all':
        return PUBLISHERS

    try:
        indices = [int(x.strip()) - 1 for x in choice.split(",")]
        selected = [PUBLISHERS[i] for i in indices if 0 <= i < len(PUBLISHERS)]
        print(f"\nì„ íƒë¨: {', '.join([p['name'] for p in selected])}")
        return selected
    except:
        print("ì˜ëª»ëœ ì…ë ¥. ì „ì²´ ì¶œíŒì‚¬ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
        return PUBLISHERS


def save_to_db(products):
    """DBì— ì €ì¥"""
    print("\n" + "="*60)
    print("ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥")
    print("="*60)

    db = SessionLocal()
    saved = 0
    skipped = 0

    try:
        for product in products:
            if not product.get("isbn"):
                skipped += 1
                continue

            # ì¤‘ë³µ ì²´í¬
            existing = db.query(KyoboProduct).filter(
                KyoboProduct.isbn == product["isbn"]
            ).first()

            if existing:
                print(f"â­ï¸  ì´ë¯¸ ì¡´ì¬: {product['title'][:40]}")
                skipped += 1
                continue

            # ì €ì¥
            kyobo_product = KyoboProduct(
                isbn=product["isbn"],
                title=product["title"],
                author=product["author"],
                publisher=product["publisher"],
                original_price=product["original_price"],
                category=product["category"],
                subcategory=product["subcategory"],
                image_url=product["image_url"],
                description=product["description"],
                kyobo_url=product["kyobo_url"],
                publish_date=product["publish_date"],
                crawled_at=datetime.utcnow(),
                is_processed=False
            )

            db.add(kyobo_product)
            saved += 1
            print(f"âœ“ ì €ì¥: {product['title'][:40]}")

        db.commit()

        print(f"\nâœ… DB ì €ì¥ ì™„ë£Œ!")
        print(f"   ì €ì¥: {saved}ê°œ")
        print(f"   ê±´ë„ˆëœ€: {skipped}ê°œ")

    except Exception as e:
        print(f"\nâŒ DB ì €ì¥ ì˜¤ë¥˜: {e}")
        db.rollback()

    finally:
        db.close()


def generate_csvs(products):
    """CSV ìƒì„±"""
    print("\n" + "="*60)
    print("ì¿ íŒ¡ CSV ìƒì„±")
    print("="*60)

    # ê³„ì • ì¡°íšŒ
    db = SessionLocal()
    accounts = db.query(Account).filter(Account.is_active == True).all()

    if accounts:
        account_names = [acc.account_name for acc in accounts]
    else:
        account_names = ["account_1", "account_2", "account_3", "account_4", "account_5"]

    db.close()

    # ìƒí’ˆ ë°ì´í„° ë³€í™˜
    product_data = []
    for p in products:
        if not p.get("isbn"):
            continue

        sale_price = int(p["original_price"] * 0.9)
        product_data.append({
            "product_name": p["title"],
            "original_price": p["original_price"],
            "sale_price": sale_price,
            "isbn": p["isbn"],
            "publisher": p["publisher"],
            "author": p["author"],
            "main_image_url": p["image_url"],
            "description": p["description"] or "ìƒì„¸í˜ì´ì§€ ì°¸ì¡°"
        })

    if not product_data:
        print("\nìƒì„±í•  ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # CSV ìƒì„±
    generator = CoupangCSVGenerator()
    result = generator.generate_batch_csvs(product_data, account_names)

    print(f"\nâœ… CSV ìƒì„± ì™„ë£Œ:")
    for account, filepath in result.items():
        print(f"   {account}: {filepath}")

    print("\n" + "="*60)
    print("ì™„ë£Œ!")
    print("="*60)
    print("\në‹¤ìŒ ë‹¨ê³„:")
    print("1. data/uploads/ í´ë” í™•ì¸")
    print("2. ì¿ íŒ¡ íŒë§¤ìì„¼í„° > ìƒí’ˆê´€ë¦¬ > ì¼ê´„ë“±ë¡")
    print("3. CSV íŒŒì¼ ì—…ë¡œë“œ")
    print()


if __name__ == "__main__":
    main()
